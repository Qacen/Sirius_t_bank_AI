{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-11-03T19:15:32.121758Z","iopub.status.busy":"2024-11-03T19:15:32.120952Z","iopub.status.idle":"2024-11-03T19:16:14.832594Z","shell.execute_reply":"2024-11-03T19:16:14.831792Z","shell.execute_reply.started":"2024-11-03T19:15:32.121719Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-11-03 19:16:10,586\tINFO util.py:124 -- Outdated packages:\n","  ipywidgets==7.7.1 found, needs ipywidgets>=8\n","Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"]}],"source":["import os\n","import gc\n","import ctypes\n","import warnings\n","\n","import torch\n","from vllm import LLM, SamplingParams"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-11-03T19:16:14.834605Z","iopub.status.busy":"2024-11-03T19:16:14.833962Z","iopub.status.idle":"2024-11-03T19:16:14.839023Z","shell.execute_reply":"2024-11-03T19:16:14.838273Z","shell.execute_reply.started":"2024-11-03T19:16:14.834570Z"},"trusted":true},"outputs":[],"source":["warnings.simplefilter('ignore')\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]   = \"0,1,2,3\"\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","def clean_memory(deep=False):\n","    gc.collect()\n","    if deep:\n","        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-11-03T19:16:39.746593Z","iopub.status.busy":"2024-11-03T19:16:39.746176Z","iopub.status.idle":"2024-11-03T19:16:55.150183Z","shell.execute_reply":"2024-11-03T19:16:55.149272Z","shell.execute_reply.started":"2024-11-03T19:16:39.746540Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: einops in /kaggle/usr/lib/vllm_installation_fix (0.8.0)\n","Collecting decord\n","  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n","Requirement already satisfied: numpy>=1.14.0 in /kaggle/usr/lib/vllm_installation_fix (from decord) (1.26.4)\n","Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n","\u001b[?25hInstalling collected packages: decord\n","Successfully installed decord-0.6.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install einops decord"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-03T19:44:15.255884Z","iopub.status.busy":"2024-11-03T19:44:15.255024Z","iopub.status.idle":"2024-11-03T19:50:52.603102Z","shell.execute_reply":"2024-11-03T19:50:52.602310Z","shell.execute_reply.started":"2024-11-03T19:44:15.255846Z"},"trusted":true},"outputs":[],"source":["import math\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","\n","def split_model(model_name):\n","    device_map = {}\n","    world_size = torch.cuda.device_count()\n","    num_layers = {\n","        'InternVL2-1B': 24, 'InternVL2-2B': 24, 'InternVL2-4B': 32, 'InternVL2-8B': 32,\n","        'InternVL2-26B': 48, 'InternVL2-40B': 60, 'InternVL2-Llama3-76B': 80}[model_name]\n","    # Since the first GPU will be used for ViT, treat it as half a GPU.\n","    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n","    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n","    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n","    layer_cnt = 0\n","    for i, num_layer in enumerate(num_layers_per_gpu):\n","        for j in range(num_layer):\n","            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n","            layer_cnt += 1\n","    device_map['vision_model'] = 0\n","    device_map['mlp1'] = 0\n","    device_map['language_model.model.tok_embeddings'] = 0\n","    device_map['language_model.model.embed_tokens'] = 0\n","    device_map['language_model.output'] = 0\n","    device_map['language_model.model.norm'] = 0\n","    device_map['language_model.lm_head'] = 0\n","    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n","\n","    return device_map\n","\n","path = \"OpenGVLab/InternVL2-8B\"\n","device_map = split_model('InternVL2-8B')\n","model = AutoModel.from_pretrained(\n","    path,\n","    torch_dtype=torch.bfloat16,\n","    low_cpu_mem_usage=True,\n","    use_flash_attn=True,\n","    trust_remote_code=True,\n","    device_map=device_map).eval()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-11-03T19:55:22.345596Z","iopub.status.busy":"2024-11-03T19:55:22.345250Z","iopub.status.idle":"2024-11-03T19:55:22.895503Z","shell.execute_reply":"2024-11-03T19:55:22.894674Z","shell.execute_reply.started":"2024-11-03T19:55:22.345563Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import torchvision.transforms as T\n","from decord import VideoReader, cpu\n","from PIL import Image\n","from torchvision.transforms.functional import InterpolationMode\n","from transformers import AutoModel, AutoTokenizer\n","# path = 'OpenGVLab/InternVL2-2B'\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# model = AutoModel.from_pretrained(\n","#     path,\n","#     torch_dtype=torch.bfloat16,\n","#     low_cpu_mem_usage=True,\n","#     use_flash_attn=True,\n","#     trust_remote_code=True).eval()\n","# model.to(device)\n","tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-11-03T20:00:00.047128Z","iopub.status.busy":"2024-11-03T20:00:00.046319Z","iopub.status.idle":"2024-11-03T20:00:04.209254Z","shell.execute_reply":"2024-11-03T20:00:04.208454Z","shell.execute_reply.started":"2024-11-03T20:00:00.047091Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Assistant: ```json\n","{\n","  \"description\": [\n","    \"long-sleeve white sweatshirt with a crew neck\",\n","    \"necklace with small spherical beads\",\n","    \"dark grey jeans\"\n","  ]\n","}\n","```\n"]}],"source":["\n","IMAGENET_MEAN = (0.485, 0.456, 0.406)\n","IMAGENET_STD = (0.229, 0.224, 0.225)\n","\n","\n","#обработка входного изображения\n","\n","def build_transform(input_size):\n","    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n","    transform = T.Compose([\n","        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n","        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n","        T.ToTensor(),\n","        T.Normalize(mean=MEAN, std=STD)\n","    ])\n","    return transform\n","\n","def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n","    best_ratio_diff = float('inf')\n","    best_ratio = (1, 1)\n","    area = width * height\n","    for ratio in target_ratios:\n","        target_aspect_ratio = ratio[0] / ratio[1]\n","        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n","        if ratio_diff < best_ratio_diff:\n","            best_ratio_diff = ratio_diff\n","            best_ratio = ratio\n","        elif ratio_diff == best_ratio_diff:\n","            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n","                best_ratio = ratio\n","    return best_ratio\n","\n","def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n","    orig_width, orig_height = image.size\n","    aspect_ratio = orig_width / orig_height\n","\n","    target_ratios = set(\n","        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n","        i * j <= max_num and i * j >= min_num)\n","    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n","    target_aspect_ratio = find_closest_aspect_ratio(\n","        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n","\n","    target_width = image_size * target_aspect_ratio[0]\n","    target_height = image_size * target_aspect_ratio[1]\n","    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n","\n","    resized_img = image.resize((target_width, target_height))\n","    processed_images = []\n","    for i in range(blocks):\n","        box = (\n","            (i % (target_width // image_size)) * image_size,\n","            (i // (target_width // image_size)) * image_size,\n","            ((i % (target_width // image_size)) + 1) * image_size,\n","            ((i // (target_width // image_size)) + 1) * image_size\n","        )\n","        split_img = resized_img.crop(box)\n","        processed_images.append(split_img)\n","    assert len(processed_images) == blocks\n","    if use_thumbnail and len(processed_images) != 1:\n","        thumbnail_img = image.resize((image_size, image_size))\n","        processed_images.append(thumbnail_img)\n","    return processed_images\n","\n","def load_image(image_file, input_size=448, max_num=12):\n","    image = Image.open(image_file).convert('RGB')\n","    transform = build_transform(input_size=input_size)\n","    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n","    pixel_values = [transform(image) for image in images]\n","    pixel_values = torch.stack(pixel_values)\n","    return pixel_values\n","\n","pixel_values = load_image('/kaggle/input/sirius-ai-dataset/images/images/img_bokoicv.jpg', max_num=12).to(torch.bfloat16).to(device)\n","generation_config = dict(max_new_tokens=128, do_sample=True)\n","\n","\n","\n","prompt = '''<image>\\n \n","you are an internationally awarded specialist in clothing recognition\n","prompt generation rules\n","1) you must carefully analyze the image\n","2) you must identify all the elements of clothing\n","3) you must describe only the basic properties of each item of clothing\n","4) you must return the response in json format, where the key is description,\n","and the value is an array of descriptions of clothing items\n","5) the answer must contain information about clothes\n","6) you must follow the rules'''\n","\n","response = model.chat(tokenizer, pixel_values, prompt, generation_config)\n","print(f'\\nAssistant: {response}')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":9869096,"sourceId":86023,"sourceType":"competition"},{"datasetId":5983894,"sourceId":9769824,"sourceType":"datasetVersion"},{"datasetId":5998303,"sourceId":9789306,"sourceType":"datasetVersion"},{"sourceId":196750896,"sourceType":"kernelVersion"},{"modelId":76277,"modelInstanceId":72256,"sourceId":104453,"sourceType":"modelInstanceVersion"},{"modelId":124969,"modelInstanceId":100788,"sourceId":119837,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
