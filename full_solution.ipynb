{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "блокнот запускался на четырех L4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import ctypes\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настраиваем окружение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T13:13:54.559092Z",
     "iopub.status.busy": "2024-11-06T13:13:54.557851Z",
     "iopub.status.idle": "2024-11-06T13:13:54.563828Z",
     "shell.execute_reply": "2024-11-06T13:13:54.563129Z",
     "shell.execute_reply.started": "2024-11-06T13:13:54.559050Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]   = \"0,1,2,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['CMAKE_ARGS'] = \"-DGGML_CUDA=on\"\n",
    "\n",
    "def clean_memory(deep=False):\n",
    "    gc.collect()\n",
    "    if deep:\n",
    "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install einops decord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем device_map, разделяем слои по гпу, что бы избежать проблемы с тензорами на разных устройствах первый и последний слой ставим на одно устройство"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T13:14:58.191001Z",
     "iopub.status.busy": "2024-11-06T13:14:58.190563Z",
     "iopub.status.idle": "2024-11-06T13:14:58.201315Z",
     "shell.execute_reply": "2024-11-06T13:14:58.200629Z",
     "shell.execute_reply.started": "2024-11-06T13:14:58.190965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def split_model(model_name):\n",
    "    device_map = {}\n",
    "    world_size = torch.cuda.device_count()\n",
    "    num_layers = {\n",
    "        'InternVL2-1B': 24, 'InternVL2-2B': 24, 'InternVL2-4B': 32, 'InternVL2-8B': 32,\n",
    "        'InternVL2-26B': 48, 'InternVL2-40B': 60, 'InternVL2-Llama3-76B': 80}[model_name]\n",
    "\n",
    "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "    layer_cnt = 0\n",
    "    for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "        for j in range(num_layer):\n",
    "            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "            layer_cnt += 1\n",
    "    device_map['vision_model'] = 0\n",
    "    device_map['mlp1'] = 0\n",
    "    device_map['language_model.model.tok_embeddings'] = 0\n",
    "    device_map['language_model.model.embed_tokens'] = 0\n",
    "    device_map['language_model.output'] = 0\n",
    "    device_map['language_model.model.norm'] = 0\n",
    "    device_map['language_model.lm_head'] = 0\n",
    "    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "\n",
    "    return device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "загружаем internVL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "path = \"OpenGVLab/InternVL2-4B\"\n",
    "device_map = split_model('InternVL2-4B')\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device_map).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведение изображение к определенному формату, подгон изображения к определенному соотношению сторон"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T13:28:28.991072Z",
     "iopub.status.busy": "2024-11-06T13:28:28.990690Z",
     "iopub.status.idle": "2024-11-06T13:28:29.004193Z",
     "shell.execute_reply": "2024-11-06T13:28:29.003518Z",
     "shell.execute_reply.started": "2024-11-06T13:28:28.991041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "#обработка входного изображения\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = image_file.convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "загрузка квантованой qwen2.5 с помощью vllm, резервируем для нее часть каждой гпу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/72b-instruct-awq/1\"\n",
    "llm = LLM(\n",
    "    model_path,\n",
    "    dtype=\"half\",                \n",
    "    max_num_seqs=8,            \n",
    "    max_model_len=1024,          \n",
    "    trust_remote_code=True,      \n",
    "    tensor_parallel_size=4,      \n",
    "    gpu_memory_utilization=0.675,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "найстройка параметров генерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T13:33:36.955039Z",
     "iopub.status.busy": "2024-11-06T13:33:36.954739Z",
     "iopub.status.idle": "2024-11-06T13:33:36.958879Z",
     "shell.execute_reply": "2024-11-06T13:33:36.958190Z",
     "shell.execute_reply.started": "2024-11-06T13:33:36.955009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0.75,            \n",
    "    seed=1,                   \n",
    "    skip_special_tokens=False,     \n",
    "    max_tokens = 256,\n",
    "    min_tokens = 32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install pyTelegramBotAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель, которая определяет начальный тип задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"cointegrated/rubert-base-cased-nli-threeway\")\n",
    "text_classifier = AutoModelForSequenceClassification.from_pretrained(\"cointegrated/rubert-base-cased-nli-threeway\")\n",
    "image_classifier = pipeline(task=\"zero-shot-image-classification\", model=\"google/siglip-so400m-patch14-384\")\n",
    "\n",
    "candidate_labels_ru = [\"одежда\", \"продукт\", \"цветы\", \"машины\", \"другое\"]\n",
    "candidate_labels_eng = [\"clothes\", \"product\", \"flowers\", \"cars\", \"another\"]\n",
    "\n",
    "def zero_shot_classification(sequence_to_classify, image_to_classify):\n",
    "    tokens = tokenizer_bert([sequence_to_classify] * len(candidate_labels_ru), candidate_labels_ru, truncation=True, return_tensors='pt', padding=True)\n",
    "    with torch.inference_mode():\n",
    "        result = torch.softmax(text_classifier(**tokens.to(text_classifier.device)).logits, -1)\n",
    "    proba = result[:, 0].cpu().numpy()\n",
    "    proba /= sum(proba)\n",
    "    label_idx = proba.argmax()\n",
    "    if label_idx == 4:\n",
    "        image_output = image_classifier(image_to_classify, candidate_labels=candidate_labels_eng)\n",
    "        label = image_output[0]['label']\n",
    "        label_idx = candidate_labels_eng.index(label)\n",
    "    return label_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T10:57:33.415655Z",
     "iopub.status.busy": "2024-11-06T10:57:33.415025Z",
     "iopub.status.idle": "2024-11-06T10:57:33.420349Z",
     "shell.execute_reply": "2024-11-06T10:57:33.419478Z",
     "shell.execute_reply.started": "2024-11-06T10:57:33.415614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def logging(bot, message, text, is_logging=True):\n",
    "    if(is_logging):\n",
    "        bot.send_message(message.chat.id, \"[Log]\"+ text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "обрабатываем изображение с помощью internVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T08:32:15.528858Z",
     "iopub.status.busy": "2024-11-06T08:32:15.528540Z",
     "iopub.status.idle": "2024-11-06T08:32:15.535478Z",
     "shell.execute_reply": "2024-11-06T08:32:15.534819Z",
     "shell.execute_reply.started": "2024-11-06T08:32:15.528828Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def request_to_VLM_internvl(img,prompt_type=1):\n",
    "    pixel_values = load_image(img, max_num=12).to(torch.bfloat16).to(device)\n",
    "    generation_config = dict(max_new_tokens=128, do_sample=True)\n",
    "    \n",
    "    prompt = {}\n",
    "    prompt[0] = '''<image>\\n \n",
    "    you are an internationally awarded specialist in clothing recognition\n",
    "    prompt generation rules\n",
    "    1) you must carefully analyze the image\n",
    "    2) you must identify all the elements of clothing\n",
    "    3) you must describe only the basic properties of each item of clothing\n",
    "    4) you must return the response in json format, where the key is description,\n",
    "    and the value is an array of descriptions of clothing items\n",
    "    5) the answer must contain information about clothes\n",
    "    6) you must follow the rules'''\n",
    "\n",
    "    prompt[1] = '''you have to assume what is most likely in the image, then make a description of the main properties of this object'''\n",
    "    \n",
    "    response = model.chat(tokenizer, pixel_values, prompt[prompt_type], generation_config)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "запрос к qwen2.5 72B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T07:51:53.431360Z",
     "iopub.status.busy": "2024-11-06T07:51:53.430567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def response_to_llm_qwen(prompt):\n",
    "    msgs = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    response = llm.chat(msgs, sampling_params, use_tqdm=False)\n",
    "    \n",
    "    return response[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсер перекрестка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T10:57:33.434206Z",
     "iopub.status.busy": "2024-11-06T10:57:33.433844Z",
     "iopub.status.idle": "2024-11-06T10:57:33.667650Z",
     "shell.execute_reply": "2024-11-06T10:57:33.666957Z",
     "shell.execute_reply.started": "2024-11-06T10:57:33.434162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def clean_price(price):\n",
    "    return re.sub(r\"[^\\d.,]\", \"\", price)\n",
    "\n",
    "def get_products_text(query, num_of_products=10):\n",
    "    response = requests.get(f'https://www.perekrestok.ru/cat/search?search={query}')\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    links = ['https://www.perekrestok.ru' + link['href'] for link in soup.find_all('a', class_='product-card__link')]\n",
    "    prices = [clean_price(price.text) for price in soup.find_all('div', class_='price-new')]\n",
    "    titles = [title.text for title in soup.find_all('div', class_='product-card__title')]\n",
    "    \n",
    "    text = ''\n",
    "    for link, price, title in list(zip(links, prices, titles))[:num_of_products]:\n",
    "        text += f'ТОВАР:\\nНазвание - {title}\\nЦена - {price} руб.\\nСсылка - {link}\\n\\n'    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа с изображением с помощью gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T12:37:32.230530Z",
     "iopub.status.busy": "2024-11-06T12:37:32.229310Z",
     "iopub.status.idle": "2024-11-06T12:37:32.244499Z",
     "shell.execute_reply": "2024-11-06T12:37:32.243474Z",
     "shell.execute_reply.started": "2024-11-06T12:37:32.230484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import mimetypes\n",
    "import os\n",
    "import json\n",
    "def request_to_VLM(img,promt_type):\n",
    "    prompt = {}\n",
    "    prompt[0] = '''<image>\\n \n",
    "    you are an internationally awarded specialist in clothing recognition\n",
    "    prompt generation rules\n",
    "    1) you must carefully analyze the image\n",
    "    2) you must identify all the elements of clothing\n",
    "    3) you must describe only the basic properties of each item of clothing\n",
    "    4) you must return the response in json format, where the key is description,\n",
    "    and the value is an array of descriptions of clothing items\n",
    "    5) the answer must contain information about clothes\n",
    "    6) you must follow the rules\n",
    "    7) ты должен дать ответ на английском'''\n",
    "\n",
    "    prompt[1] = '''you have to assume what is most likely in the image, then make a description of the main properties of this object, ты должен дать ответ на английском'''\n",
    "    \n",
    "    BASE_URL = \"https://generativelanguage.googleapis.com/\"  \n",
    "    GOOGLE_API_KEY = \"апи ключ\"\n",
    "    DISPLAY_NAME = \"Sample Image\"\n",
    "    \n",
    "    #  Загрузка тестового изображения\n",
    "    IMG_PATH_2 = \"sample_image.jpg\"\n",
    "    img.save(IMG_PATH_2, format=\"JPEG\")\n",
    "    \n",
    "    # Определение MIME-типа и размера файла\n",
    "    mime_type = mimetypes.guess_type(IMG_PATH_2)[0]\n",
    "    num_bytes = os.path.getsize(IMG_PATH_2)\n",
    "    \n",
    "    # инициализация загрузки для получения URL\n",
    "    \n",
    "    headers = {\n",
    "        \"X-Goog-Upload-Protocol\": \"resumable\",\n",
    "        \"X-Goog-Upload-Command\": \"start\",\n",
    "        \"X-Goog-Upload-Header-Content-Length\": str(num_bytes),\n",
    "        \"X-Goog-Upload-Header-Content-Type\": mime_type,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\"file\": {\"display_name\": DISPLAY_NAME}}\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{BASE_URL}/upload/v1beta/files?key={GOOGLE_API_KEY}\",\n",
    "        headers=headers,\n",
    "        json=data\n",
    "    )\n",
    "    \n",
    "    # Получаем URL для загрузки данных\n",
    "    upload_url = response.headers.get(\"X-Goog-Upload-URL\")\n",
    "    if not upload_url:\n",
    "        raise Exception(\"Upload URL not found in response headers\")\n",
    "    \n",
    "    # Шаг 2: Загрузка самого файла\n",
    "    with open(IMG_PATH_2, \"rb\") as file_data:\n",
    "        upload_headers = {\n",
    "            \"Content-Length\": str(num_bytes),\n",
    "            \"X-Goog-Upload-Offset\": \"0\",\n",
    "            \"X-Goog-Upload-Command\": \"upload, finalize\"\n",
    "        }\n",
    "        upload_response = requests.post(upload_url, headers=upload_headers, data=file_data)\n",
    "    \n",
    "    # Получаем URI загруженного файла из ответа\n",
    "    file_info = upload_response.json()\n",
    "    file_uri = file_info.get(\"file\", {}).get(\"uri\")\n",
    "    if not file_uri:\n",
    "        raise Exception(\"File URI not found in upload response\")\n",
    "    \n",
    "    # Генерация ответа с использованием загруженного файла\n",
    "    content_request = {\n",
    "        \"contents\": [{\n",
    "            \"parts\": [\n",
    "                {\"text\": prompt[promt_type]},\n",
    "                {\"file_data\": {\"mime_type\": mime_type, \"file_uri\": file_uri}}\n",
    "            ]\n",
    "        }],\n",
    "    'generationConfig': {\n",
    "        'stopSequences': [\n",
    "            'Title',\n",
    "        ],\n",
    "        'temperature': 0.7,\n",
    "        'maxOutputTokens': 256,\n",
    "        'topP': 0.8,\n",
    "        'topK': 10,\n",
    "    },\n",
    "    }\n",
    "    \n",
    "    content_headers = {\"Content-Type\": \"application/json\"}\n",
    "    content_response = requests.post(\n",
    "        f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GOOGLE_API_KEY}\",\n",
    "        headers=content_headers,\n",
    "        json=content_request\n",
    "    )\n",
    "    \n",
    "    response_data = content_response.json()\n",
    "    \n",
    "    # Извлечение текста из кандидатов\n",
    "    return response_data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "работа с генерацией текста через gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T12:25:39.718753Z",
     "iopub.status.busy": "2024-11-06T12:25:39.718019Z",
     "iopub.status.idle": "2024-11-06T12:25:41.553462Z",
     "shell.execute_reply": "2024-11-06T12:25:41.552565Z",
     "shell.execute_reply.started": "2024-11-06T12:25:39.718712Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"task\": \"Ты должен порекомендовать пользователю несколько вариантов блюд или продуктов, которые можно съесть с вареной колбасой. Учитывай, что колбаса на изображении - докторская.\",\\n  \"subtask\": \"Ты должен кратко описать, почему эти варианты сочетаются с вареной колбасой. Например, можно упомянуть о вкусовых сочетаниях, традиционных комбинациях или возможности создания легкого перекуса или полноценного обеда.\"\\n}\\n``` \\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def response_to_LLM(text):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        'key': 'апи ключ',\n",
    "    }\n",
    "    \n",
    "    json_data = {\n",
    "        'contents': [\n",
    "            {\n",
    "                'parts': [\n",
    "                    {\n",
    "                        'text': text,\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    'generationConfig': {\n",
    "        'stopSequences': [\n",
    "            'Title',\n",
    "        ],\n",
    "        'temperature': 0.7,\n",
    "        'maxOutputTokens': 512,\n",
    "        'topP': 0.8,\n",
    "        'topK': 10,\n",
    "    },\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent',\n",
    "        params=params,\n",
    "        headers=headers,\n",
    "        json=json_data,\n",
    "    )\n",
    "    return response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "prompt = '''ты должен шоппинг ассистент и твоя задача предложить пользователю какие-то товары, запросы пользователя могут координально отличатся, поэтому ты должен определить задачу для llm модели, твой ответ должен состоять из 2 частей: task (с помощью этого запроса модель определит какие товары нужно найти), subtask(нужен для генерации финального ответа после определения списка товаров, с помощью этой задачи модель предлагает пользователю товары и выполяет просьбу в его запросе)\n",
    "пример: 'что я могу съесть вместе с этим? на изображении находится хлеб' ты должен ответить так  {\"task\": \"ты должен порекомендовать пользователю несколько вариантов еды или напитков, с которыми можно съесть хлеб\" ,\"subtask\": \"ты должен кратко рассказать почему пользователю нужны эти товары\"}, пример: пользователь 'я хочу приготовить плов', тогда твой ответ это: {\"task\":\"сделай список товаров, из которого состоит плов\", \"subtask\":\"ты должен рассказать пользователю рецепт\" } вот информацию про изображение и запрос пользователя: \n",
    "текст:Что я могу съесть с этим?\n",
    "тип задачи: продукт\n",
    "ответ VLM: The image shows a sliced sausage. It is a boiled sausage, also known as a cooked sausage. It is a cylindrical shape, with a pink color and a smooth texture. The sausage is wrapped in a plastic casing. The label on the sausage indicates that it is a \"Doctor's\" sausage, which is a type of boiled sausage that is typically made with pork and beef. The sausage is also labeled as being \"GOST\" certified, which means that it meets Russian quality standards.'''\n",
    "response_to_LLM(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Форматирование генерации ллмок в json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T12:26:20.750405Z",
     "iopub.status.busy": "2024-11-06T12:26:20.750043Z",
     "iopub.status.idle": "2024-11-06T12:26:22.066786Z",
     "shell.execute_reply": "2024-11-06T12:26:22.066020Z",
     "shell.execute_reply.started": "2024-11-06T12:26:20.750372Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'Найди список продуктов, которые можно съесть с варёной колбасой, учитывая, что пользователь - мужчина, холостой, 35 лет и с доходом выше среднего.', 'subtask': \"Предложи несколько вариантов блюд, которые можно приготовить с варёной колбасой, учитывая вкусовые предпочтения мужчины.  Опиши каждый вариант кратко, например, 'Это классическое сочетание, которое идеально подходит для быстрого перекуса.  ' или 'Это более изысканный вариант, который идеально подходит для ужина.' \"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def json_format(text):\n",
    "    if (len(text.split('```'))>1):\n",
    "        text = text.split('```')[1].replace('json','')\n",
    "    return json.loads(text)\n",
    "pr ='''\n",
    "информация про пользователя: мужчина, возраст 35 лет, холостой, доход выше среднего\n",
    "текст:Что я могу съесть с этим?\n",
    "тип задачи: продукт\n",
    "ответ VLM: The image shows a sliced sausage. It is a boiled sausage, also known as a cooked sausage. It is a cylindrical shape, with a pink color and a smooth texture. The sausage is wrapped in a plastic casing. The label on the sausage indicates that it is a \"Doctor's\" sausage, which is a type of boiled sausage that is typically made with pork and beef. The sausage is also labeled as being \"GOST\" certified, which means that it meets Russian quality standards.'''\n",
    "print(json_format(response_to_LLM(prompt_llm(1) + pr)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "промты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T12:55:39.842373Z",
     "iopub.status.busy": "2024-11-06T12:55:39.841971Z",
     "iopub.status.idle": "2024-11-06T12:55:39.849907Z",
     "shell.execute_reply": "2024-11-06T12:55:39.849002Z",
     "shell.execute_reply.started": "2024-11-06T12:55:39.842329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prompt_llm(index):\n",
    "    prompts = [\n",
    "'''\n",
    "правила генерации ответа\n",
    "        1) ты должен должен подобрать список продуктов, наиболее подходящих под запрос пользователя\n",
    "        2) ты должен сформировать поисковый запрос для сайта магазина для каждого проодукта на русском, \n",
    "        3) поисковый запрос должен быть простым, нельзя использовать сложные формулировки и спец символы, в одном запросе должен быть один продукт\n",
    "        4) поисковой запрос должен быть на русском языке\n",
    "        5) ответ должен быть в формате json в виде {\"search_offers\": [{\"item\": \"запрос\"},{\"item\": \"запрос\"},{\"item\": \"запрос\"}]}\n",
    "        6) ответ должен состоять только из ответа в формате \n",
    "        7) ты должен соблюдать правила генерации ответа\n",
    "''',\n",
    "'''ты шоппинг ассистент, твоя задача предложить пользователю какие-то товары, запросы пользователя могут координально отличатся, поэтому ты должен определить задачу для llm модели, твой ответ должен состоять из 2 частей: task (с помощью этого запроса модель определит какие товары нужно найти), subtask(нужен для того, что бы модель опираясь на найденные товары сгенерировала ответ для пользователя)\n",
    "пример: 'что я могу съесть вместе с этим? на изображении находится хлеб' ты должен ответить так  {\"task\": \"ты должен порекомендовать пользователю несколько вариантов еды или напитков, с которыми можно съесть хлеб\" ,\"subtask\": \"ты должен кратко рассказать почему пользователю нужны эти товары\"}, пример: пользователь 'я хочу приготовить плов', тогда твой ответ это: {\"task\":\"сделай список товаров, из которого состоит плов\", \"subtask\":\"ты должен рассказать пользователю рецепт\" } \n",
    "ты должен дать ответ в заданном формате, даже если информации мало, задача по умолчанию это рекомендация наиболее подходящих товаров\n",
    "вот информацию про изображение и запрос пользователя: \n",
    "''','''\n",
    "ты шоппинг ассистент\n",
    "ты должен сформировать финальный ответ, обрати внимание на запрос пользователя, информацию про него, найденные для его запроса товары и информацию про изображения\n",
    "твой ответ должен быть емким, делай содержание небольшим, но оно должно сохранить основную суть\n",
    "при предложении товаров, ты должен прикреплять основную информацию и ссылки, помимо этого можешь укахывать цену\n",
    "вот информация про пользователя:\n",
    "\n",
    "''']\n",
    "    \n",
    "    return prompts[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "бот (может работать и с локальными ллмками)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import telebot\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "is_logging = False\n",
    "user_info = 'мужчина, возраст 35 лет, холостой, доход выше среднего'\n",
    "\n",
    "bot = telebot.TeleBot('апи ключ')\n",
    "# Обработчик команды /start\n",
    "@bot.message_handler(commands=['start'])\n",
    "def send_welcome(message):\n",
    "    bot.send_message(message.chat.id, \"Привет! Я шопинг ассистент\\nТеперь у меня есть глаза, пока я могу рекомендовать еду или бытовые товары\\nОтправь фото и сформулируй вопрос, я найду именно то, что тебе нужно)\\n\\nВся информация о боте в /info\")\n",
    "\n",
    "@bot.message_handler(commands=['info'])\n",
    "def info(message):\n",
    "    bot.send_message(message.chat.id, \"Наш бот нужен для того, что бы консультировать пользователей и предлагать им товары\\n\\nДля того, что бы получить ответ, нужно загрузить фото и сформулировать запрос в описании к этому фото\\n\\n В данный момент он работает с едой и некоторыми домашними товарами, мы интегрировали его только с перекрестком\\n\\nОсновное решение использует internvl2-8B и квантованный Qwen2-72B, нам удалось локально захостить обе модели одновременно в блокноте кагл на четырех L4\\n\\nПомимо этого, мы интегрировали их в телеграмм бота, но для лучшей точности и надеждности, бот все еще работает на api gemini. (реализацию решения мы загрузили в наш гитхаб https://github.com/Qacen/Sirius_t_bank_AI)\\n\\nДалее в рамках нашей задачи, мы реализовали логику, позволяющую модели в несколько этапов сгенерировать ответ\\nДля большей персонализации, мы храним информацию про пользователя в llm, пример такой информации вы можете найти в \\n/user_info \\nВ перспективе создание памяти(сильная суммаризация запросов пользователя)\\nКак работает ассистент: изначально VLM анализирует изображение и создает его описание\\n\\nЗатем мы на основании всех имеющихся факторов создаем промпт в llm и просим сформулировать что от нас хочет пользователь\\nПри большой вариантивности задач это значительно улучшает качество ответа\\nСледующим шагом является определение списка товаров в формате json\\nИспользуя поиск на сайте перекресток мы парсим нужные нам товары\\nНаконец, llm формирует финальный ответ и мы отправляем его пользователю со списком найденных товаров\\n\\nДля того, что бы изменить информацию о пользователе используйте команду \\n/user_info_change\\n\\n/log_on - включение логов\\n/log_off - отключение логов\")\n",
    "\n",
    "@bot.message_handler(commands=['user_info'])\n",
    "def handle_user_info(message):\n",
    "    global user_info\n",
    "    bot.send_message(message.chat.id, \"в данный момент я имею такую информацию про пользователя:\\n\\n\"+user_info)\n",
    "# Обработчик любых других сообщений\n",
    "@bot.message_handler(commands=['user_info_change'])\n",
    "def handle_user_info(message):\n",
    "    bot.send_message(message.chat.id, \"Пожалуйста, введите информацию, которую вы хотите сохранить:\")\n",
    "    bot.register_next_step_handler(message, save_user_info)\n",
    "    \n",
    "@bot.message_handler(commands=['log_on'])\n",
    "def handle_lof_on(message):\n",
    "    global is_logging\n",
    "    is_logging = True\n",
    "    print(is_logging)\n",
    "    bot.send_message(message.chat.id, \"логи включены\")\n",
    "    \n",
    "@bot.message_handler(commands=['log_off'])\n",
    "def handle_log_off(message):\n",
    "    global is_logging\n",
    "    is_logging = False\n",
    "    bot.send_message(message.chat.id, \"логи отключены\")\n",
    "\n",
    "\n",
    "def save_user_info(message):\n",
    "    # Сохраняем введенную информацию в словарь\n",
    "    global user_info\n",
    "    user_info = message.text\n",
    "    bot.send_message(message.chat.id, \"Информация успешно сохранена!\")\n",
    "\n",
    "# Обработчик сообщений с фото\n",
    "@bot.message_handler(content_types=['photo'])\n",
    "def handle_photo(message):\n",
    "    global user_info\n",
    "    global is_logging\n",
    "    bot.send_message(message.chat.id, \"Отлично, подожди пару минут, мне нужно подумать и дать тебе лучший ответ!(перед началом работы ознакомься с ботом /info)\")\n",
    "    \n",
    "    # получаем фото\n",
    "    file_info = bot.get_file(message.photo[-1].file_id)\n",
    "    file = requests.get(f\"https://api.telegram.org/file/bot{bot.token}/{file_info.file_path}\")\n",
    "\n",
    "    img = Image.open(BytesIO(file.content))\n",
    "    caption_text = message.caption if message.caption else \"\"\n",
    "\n",
    "    \n",
    "    prompt_to_LLM = '''Ты должен дать персональную рекомендацию для пользователя на основе этой информации: \\n'''\n",
    "    prompt_to_LLM += user_info\n",
    "\n",
    "    # получаем описание изображения от vlm\n",
    "    text_vlm = request_to_VLM(img,1)\n",
    "    logging(bot,message,\"ответ VLM: \"+ text_vlm,is_logging)\n",
    "        \n",
    "    # генерируем первый промпт к ллм, это нужно для более точного определения запроса пользователя\n",
    "    user_want_prompt = prompt_llm(1)\n",
    "    user_want_prompt += 'информация про пользователя: ' + user_info + '\\n запрос пользователя: ' + caption_text + 'информация из изображения: '+ text_vlm\n",
    "    user_want = json_format(response_to_LLM(user_want_prompt))\n",
    "    logging(bot,message,\"запрос пользователя: \"+ str(user_want),is_logging)\n",
    "    \n",
    "    # генерируем подборку товаров для пользователя\n",
    "    print(user_want['subtask'])\n",
    "    prompt_to_LLM += \"\\n \"+user_want['task']\n",
    "    prompt_to_LLM += prompt_llm(0)\n",
    "    prompt_to_LLM += f\"после анализа изображения мы получили такую информацию: \\n{text_vlm}\\n\"\n",
    "    text_llm = response_to_LLM(prompt_to_LLM) \n",
    "    logging(bot,message,\"ответ LLM: \"+ text_llm,is_logging)\n",
    "    form_json = json_format(text_llm)\n",
    "    logging(bot,message,\"json массив: \"+ str(form_json), is_logging)\n",
    "    \n",
    "    # парсим данные с перекрестка\n",
    "    parsing_text = ''\n",
    "    for i in form_json['search_offers']:\n",
    "       final_text += get_products_text(i['item'],1)\n",
    "    print(parsing)\n",
    "    logging(bot,message,\"найдены товары: \"+ parsing)\n",
    "    \n",
    "    # генерируем финальный ответ\n",
    "    prompt_to_final_asnwer = prompt_llm(2)\n",
    "    prompt_to_final_asnwer += \"\\n запрос пользователя\"+ user_want['subtask']\n",
    "    prompt_to_final_asnwer += '\\nинформация про пользователя: ' + user_info + '\\n запрос пользователя: ' + caption_text + 'информация из изображения: '+ text_vlm\n",
    "    prompt_to_final_asnwer += 'найденные товары: ' + text_llm\n",
    "    \n",
    "    final_answer = response_to_LLM(prompt_to_final_asnwer) \n",
    "    bot.send_message(message.chat.id, final_answer)\n",
    "\n",
    "bot.polling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример генерации ответа qwen2-72B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T15:57:45.278549Z",
     "iopub.status.busy": "2024-11-05T15:57:45.278173Z",
     "iopub.status.idle": "2024-11-05T15:58:03.566133Z",
     "shell.execute_reply": "2024-11-05T15:58:03.565212Z",
     "shell.execute_reply.started": "2024-11-05T15:57:45.278515Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ансамбли (ensemble) в машинном обучении представляют собой методы, которые позволяют объединять несколько моделей машинного обучения для улучшения точности прогнозирования и снижения вероятности переобучения. Ансамбли могут быть основаны на разных подходах, таких как бэггинг (bagging), бустинг (boosting) и стакинг (stacking). Давайте рассмотрим каждый из них подробнее.\n",
      "\n",
      "1. **Бэггинг (Bagging)**:\n",
      "   - **Описание**: Бэггинг (bootstrap aggregating) – это метод, при котором несколько экземпляров модели обучены на различных подвыборках исходного набора данных. Эти подвыборки создаются путем случайного отбора наблюдений из исходного набора данных с возвращением (bootstrap sample). После обучения каждая модель делает прогноз, и окончательное предсказание формируется путем голосования (для задач классификации) или усреднения (для задач регрессии).\n",
      "   - **Пример**: Один из самых известных алгоритмов бэггинга – случайный лес (Random Forest). В случайном лесу каждое дерево принимает решение, и окончательное решение формируется путем голосования.\n",
      "\n",
      "2. **Бустинг (Boosting)**:\n",
      "   - **Описание**: Бустинг – это метод обучения последовательности слабых моделей (например, деревьев решений с малым числом узлов), где каждая следующая модель обучается на ошибках предыдущих моделей. Этот метод позволяет улучшить точность прогнозирования, поскольку каждая новая модель компенсирует ошибки предыдущих моделей.\n",
      "   - **Пример**: AdaBoost (Adaptive Boosting) и Gradient Boosting – это популярные алгоритмы бустинга. В Gradient Boosting каждая следующая модель обучается на градиенте ошибки предыдущей модели.\n",
      "\n",
      "3. **Стакинг (Stacking)**:\n",
      "   - **Описание**: Стакинг – это метод, при котором прогнозы нескольких моделей используются в качестве входных данных для другой модели, которая делает окончательное предсказание. Эта вторичная модель (meta-model) обучается на прогнозах первичных моделей и может быть любым алгоритмом машинного обучения.\n",
      "   - **Пример**: В стакинге можно использовать различные модели машинного обучения, такие как логистическая регрессия, SVM, деревья решений и т.д. Эти модели могут быть обучены на различных подвыборках данных, и их прогнозы используются для обучения второй модели, которая делает окончательное предсказание.\n",
      "\n",
      "Ансамбли являются мощным инструментом в машинном обучении, позволяющим улучшить качество прогнозирования и устойчивость модели к шуму данных. Однако важно помнить, что использование ансамблей может увеличивать вычислительные затраты и сложность модели.\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0.45,            \n",
    "    seed=1,                   \n",
    "    skip_special_tokens=False,     \n",
    "    max_tokens = 256,\n",
    "    min_tokens = 32,\n",
    ")\n",
    "\n",
    "msgs = [\n",
    "    {\"role\": \"user\", \"content\": \"расскажи про ансамбли в машинном обучении\"}\n",
    "]\n",
    "\n",
    "response = llm.chat(msgs, sampling_params, use_tqdm=False)\n",
    "\n",
    "print(response[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T08:14:56.158262Z",
     "iopub.status.busy": "2024-11-06T08:14:56.157414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "del llm\n",
    "clean_memory(deep=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 9869096,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "datasetId": 5983894,
     "sourceId": 9769824,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5998303,
     "sourceId": 9789306,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 205183965,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 76277,
     "modelInstanceId": 72256,
     "sourceId": 104453,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 127417,
     "modelInstanceId": 103186,
     "sourceId": 122612,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 127417,
     "modelInstanceId": 118183,
     "sourceId": 139552,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
